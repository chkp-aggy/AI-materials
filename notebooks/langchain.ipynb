{"cells":[{"cell_type":"markdown","id":"0e869c38-1cca-4db7-8fda-f09c0f98b2f6","metadata":{"id":"0e869c38-1cca-4db7-8fda-f09c0f98b2f6"},"source":["# Notebook 4: LangChain basics"]},{"cell_type":"markdown","id":"12bd9a03","metadata":{},"source":["# LangChain\n","LangChain is a framework for developing applications powered by large language models (LLMs)."]},{"cell_type":"markdown","id":"b80f2d95-70ea-41d4-8493-f49e316a61df","metadata":{"id":"b80f2d95-70ea-41d4-8493-f49e316a61df"},"source":["<img src=\"https://drive.google.com/uc?id=11muIjdnCaZksAwoaFAZjL8hAH5Z0mnmb\" />"]},{"cell_type":"markdown","id":"1920eae1-f58d-4e6c-b440-fd4e76c0bb63","metadata":{"id":"1920eae1-f58d-4e6c-b440-fd4e76c0bb63"},"source":["### Step 1: Use GPT via LangChain\n","\n","* We use the AzureChatOpenAI API\n","* Working with azure is security-wise approved"]},{"cell_type":"code","execution_count":1,"id":"0eefc55a","metadata":{},"outputs":[],"source":["from langchain_openai import AzureChatOpenAI\n","\n","llm = AzureChatOpenAI(\n","    azure_deployment=\"gpt-35-turbo\",\n","    api_version=\"2023-06-01-preview\",\n","    temperature=0.7,\n","    max_tokens=None,\n","    timeout=None,\n","    max_retries=2,\n",")"]},{"cell_type":"code","execution_count":2,"id":"91fb0353-dae3-4f70-b6d0-bd535b76c505","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7385,"status":"ok","timestamp":1694039238322,"user":{"displayName":"Avner Duchovni","userId":"12000805702532876017"},"user_tz":-180},"id":"91fb0353-dae3-4f70-b6d0-bd535b76c505","outputId":"0f046db5-38c6-498b-f9e7-add31d65e2a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["To exit vim, you need to follow these steps:\n","\n","1. Press the Esc key to ensure you are in command mode.\n","2. Type `:q` to quit vim. If you have unsaved changes, vim will not exit and will display an error message.\n","3. If you have made changes and want to discard them, you can type `:q!` to forcefully quit vim without saving.\n","4. If you have made changes and want to save them before quitting, type `:wq` to write the changes to the file and exit vim.\n","\n","Remember, vim can be a bit complex for beginners. If you find yourself struggling, you can refer to vim's documentation or use other text editors that are more user-friendly.\n"]}],"source":["res = llm.invoke('Hello, how do I exit vim?')  # Send an API request to Azure OpenAI via LangChain. Simple!\n","print(res.content)  # No need for complicated parsing. Sweet."]},{"cell_type":"markdown","id":"7ef7d3fc-109f-4d02-99f5-a19e104316cd","metadata":{"id":"7ef7d3fc-109f-4d02-99f5-a19e104316cd"},"source":["### Step 2: Use a ChatPromptTemplate to create your prompt\n","\n","We can define prompt templates.\n","* These templates consist of System, Human and AI messages.\n","* The template contains multiple variables that will be given different values.\n","* The PromptTemplate is like Python's f-strings (and in fact uses it)"]},{"cell_type":"code","execution_count":3,"id":"6f6e0380-003b-4e87-94e9-481c362fb79d","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1694039238323,"user":{"displayName":"Avner Duchovni","userId":"12000805702532876017"},"user_tz":-180},"id":"6f6e0380-003b-4e87-94e9-481c362fb79d"},"outputs":[],"source":["from langchain.prompts import ChatPromptTemplate"]},{"cell_type":"code","execution_count":4,"id":"210de892-2507-4896-8785-0baf1c3c2d1a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":661,"status":"ok","timestamp":1694039238977,"user":{"displayName":"Avner Duchovni","userId":"12000805702532876017"},"user_tz":-180},"id":"210de892-2507-4896-8785-0baf1c3c2d1a","outputId":"f5c8d4fe-2b16-4d42-913f-d06589136ce2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_8414/3037245385.py:14: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n","  response = llm(messages)\n"]},{"name":"stdout","output_type":"stream","text":["My name is Check Bot. How can I assist you today?\n"]}],"source":["template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n","    (\"human\", \"Hello, how are you doing?\"),\n","    (\"assistant\", \"I'm doing well, thanks!\"),  # Poor AI, we put words in his mouth!\n","    (\"human\", \"{user_input}\"),\n","])\n","\n","# We use the Template to create an actual prompt by giving the required arguments\n","messages = template.format_messages(\n","    name=\"Check Bot\",\n","    user_input=\"What is your name?\"\n",")\n","\n","response = llm(messages)\n","print(response.content)"]},{"cell_type":"markdown","id":"f4022e77-cb6d-4666-aa87-c0635f0c6ebe","metadata":{"id":"f4022e77-cb6d-4666-aa87-c0635f0c6ebe"},"source":["### Step 3: Use Few-shot"]},{"cell_type":"markdown","id":"7529edda-cb1b-492e-b772-a4bf454b9fbc","metadata":{"id":"7529edda-cb1b-492e-b772-a4bf454b9fbc"},"source":["* When asking an LLM to do something for the first time, without any examples, we call it **zero shot**\n","* Supplying the LLM with an example for what we want can drastically increase its performance\n","* If a single example is given to the LLM, we call it **1-shot**\n","* If multiple examples are supplied, we call it **few shot**\n","* https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples_chat\n","\n","This technique is based on the [Language Models are Few-Shot Learners paper](https://arxiv.org/abs/2005.14165)"]},{"cell_type":"code","execution_count":5,"id":"f348f7d4-6db1-4412-a292-deb4c82dcd5e","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1694039238977,"user":{"displayName":"Avner Duchovni","userId":"12000805702532876017"},"user_tz":-180},"id":"f348f7d4-6db1-4412-a292-deb4c82dcd5e"},"outputs":[],"source":["from langchain.prompts import FewShotChatMessagePromptTemplate"]},{"cell_type":"code","execution_count":6,"id":"23ef53e2-c49b-42c3-971e-43b978f2479c","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694039238978,"user":{"displayName":"Avner Duchovni","userId":"12000805702532876017"},"user_tz":-180},"id":"23ef53e2-c49b-42c3-971e-43b978f2479c"},"outputs":[],"source":["# Use a list of dictionaries to store examples. Each example is a dictionary.\n","# Notice that each example has a user input and an expected result.\n","\n","examples = [\n","    {\"input\": \"2 + 2\", \"output\": \"The amazing answer is 4!\"},\n","    {\"input\": \"2 + 3\", \"output\": \"The unexpected answer is 5!\"},\n","]"]},{"cell_type":"code","execution_count":7,"id":"91f859dc-6d2f-4082-ac54-424855205a1d","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1694039238978,"user":{"displayName":"Avner Duchovni","userId":"12000805702532876017"},"user_tz":-180},"id":"91f859dc-6d2f-4082-ac54-424855205a1d"},"outputs":[],"source":["# A single Template will be used for all examples\n","\n","example_prompt_template = ChatPromptTemplate.from_messages(\n","    [\n","        (\"human\", \"{input}\"),\n","        (\"assistant\", \"{output}\"),\n","    ]\n",")"]},{"cell_type":"code","execution_count":8,"id":"270ccbc7-f737-4921-8d13-30027c64490a","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694039238979,"user":{"displayName":"Avner Duchovni","userId":"12000805702532876017"},"user_tz":-180},"id":"270ccbc7-f737-4921-8d13-30027c64490a"},"outputs":[],"source":["# We supply the FewShotChatMessagePromptTemplate with a Template and a dictionary\n","\n","few_shot_prompt = FewShotChatMessagePromptTemplate(\n","    example_prompt=example_prompt_template,  # This is how the examples should look like\n","    examples=examples,  # And this is the actual examples\n",")"]},{"cell_type":"code","execution_count":9,"id":"348df42b-5cd9-4021-8724-469a147fc497","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694039238979,"user":{"displayName":"Avner Duchovni","userId":"12000805702532876017"},"user_tz":-180},"id":"348df42b-5cd9-4021-8724-469a147fc497"},"outputs":[],"source":["# Template for the final prompt\n","prompt_template = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", \"You are a wonderous wizard of math.\"),\n","        few_shot_prompt,  # This encapsulates all the examples\n","        (\"human\", \"{input}\"),\n","    ]\n",")"]},{"cell_type":"code","execution_count":10,"id":"4471a1b8-be49-4c36-91a7-f346f2afb933","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1694039238980,"user":{"displayName":"Avner Duchovni","userId":"12000805702532876017"},"user_tz":-180},"id":"4471a1b8-be49-4c36-91a7-f346f2afb933","outputId":"24df038e-c18c-49cb-fdab-9c12119f0fe5"},"outputs":[{"data":{"text/plain":["[SystemMessage(content='You are a wonderous wizard of math.', additional_kwargs={}, response_metadata={}),\n"," HumanMessage(content='2 + 2', additional_kwargs={}, response_metadata={}),\n"," AIMessage(content='The amazing answer is 4!', additional_kwargs={}, response_metadata={}),\n"," HumanMessage(content='2 + 3', additional_kwargs={}, response_metadata={}),\n"," AIMessage(content='The unexpected answer is 5!', additional_kwargs={}, response_metadata={}),\n"," HumanMessage(content='6 + 3?', additional_kwargs={}, response_metadata={})]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["final_prompt = prompt_template.format_messages(\n","    input=\"6 + 3?\"\n",")\n","\n","final_prompt  # Let's see our prompt"]},{"cell_type":"code","execution_count":11,"id":"7bf2e782","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The sum of 3 and 6 is 9.\n"]}],"source":["# First let's see what GPT responds without examples\n","print(llm(\"3 + 6\").content)"]},{"cell_type":"code","execution_count":12,"id":"ca69816f-026b-4de1-b009-8857ffa5086f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":506,"status":"ok","timestamp":1694039239480,"user":{"displayName":"Avner Duchovni","userId":"12000805702532876017"},"user_tz":-180},"id":"ca69816f-026b-4de1-b009-8857ffa5086f","outputId":"cc62e0e1-4605-4559-ec94-080b5b1dcf63"},"outputs":[{"name":"stdout","output_type":"stream","text":["The magical answer is 9!\n"]}],"source":["response = llm(final_prompt)\n","print(response.content)"]},{"cell_type":"markdown","id":"77e77560-9de0-4c41-a83a-233e4c4bea42","metadata":{"id":"77e77560-9de0-4c41-a83a-233e4c4bea42"},"source":["## Example selectors\n","* LangChain allows you to pick examples in dynamic ways.\n","* You can select examples by length, relevance, similarity and more\n","* https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/\n"]},{"cell_type":"markdown","id":"028a6c28-9bf4-4023-bb0f-bf4f36d915f0","metadata":{"id":"028a6c28-9bf4-4023-bb0f-bf4f36d915f0"},"source":["## Exercises\n","* Use few shots and a system prompt to create a chatbot as follows -\n","* Input is a name of a country\n","* Output should be in the format of `Capital | population | gdp`"]},{"cell_type":"markdown","id":"60ac996c-8328-4f5a-8acd-7f82725834a4","metadata":{"id":"60ac996c-8328-4f5a-8acd-7f82725834a4"},"source":["## Summary\n","* We can use LangChain to communicate with OpenAI with greater ease\n","* We can use `ChatPromptTemplate` as 'recipes' for generating prompts\n","* When an LLM tries to solve a problem it has never seen before we can it **zero shot**\n","* When an LLM solves a problem with a single/multiple examples, we call it **one shot**/**few shot**\n","* We can use `FewShotChatMessagePromptTemplate` to create few shot prompts"]},{"cell_type":"markdown","id":"fe070664","metadata":{},"source":["## Resources\n","\n","* https://python.langchain.com/docs/get_started/introduction.html  # docs\n","* https://github.com/langchain-ai/langchain  # github\n","* docs: https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.azure_openai.AzureChatOpenAI.html"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":5}
